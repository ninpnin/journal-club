<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>journal-club</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <link rel="stylesheet" href="pandoc.css" />
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<h1>UU Stats and ML Journal Club</h1>
<p>This is the Machine learning and Statistics Journal Club. We gather
roughly every third week.</p>
<h2>Upcoming meetings</h2>
<h3>2023-09-14 – Active Testing <a href="#kossen2021active"
class="ref">[kossen2021active]</a></h3>
<figure>
<p><img src="img/active-testing.png" style="width:60.0%" /></p>
</figure>
<p>Abstract: We introduce a new framework for sample-efficient model
evaluation that we call active testing. While approaches like active
learning reduce the number of labels needed for model training, existing
literature largely ignores the cost of labeling test data, typically
unrealistically assuming large test sets for model evaluation. This
creates a disconnect to real applications, where test labels are
important and just as expensive, eg for optimizing hyperparameters.
Active testing addresses this by carefully selecting the test points to
label, ensuring model evaluation is sample-efficient. To this end, we
derive theoretically-grounded and intuitive acquisition strategies that
are specifically tailored to the goals of active testing, noting these
are distinct to those of active learning. As actively selecting labels
introduces a bias; we further show how to remove this bias while
reducing the variance of the estimator at the same time. Active testing
is easy to implement and can be applied to any supervised machine
learning method. We demonstrate its effectiveness on models including
WideResNets and Gaussian processes on datasets including Fashion-MNIST
and CIFAR-100.</p>
<p>Presenter: Väinö Yrjänäinen</p>
<h2>Past meetings</h2>
<h3>2023-06-08 – Using natural language and program abstractions to
instill human inductive biases in machines <a href="#kumar2022using"
class="ref">[kumar2022using]</a></h3>
<figure>
<p><img src="img/human-biases.png" style="width:90.0%" /></p>
</figure>
<p>Abstract: Strong inductive biases give humans the ability to quickly
learn to perform a variety of tasks. Although meta-learning is a method
to endow neural networks with useful inductive biases, agents trained by
meta-learning may sometimes acquire very different strategies from
humans. We show that co-training these agents on predicting
representations from natural language task descriptions and programs
induced to generate such tasks guides them toward more human-like
inductive biases. Human-generated language descriptions and program
induction models that add new learned primitives both contain abstract
concepts that can compress description length. Co-training on these
representations result in more human-like behavior in downstream
meta-reinforcement learning agents than less abstract controls
(synthetic language descriptions, program induction without learned
primitives), suggesting that the abstraction supported by these
representations is key.</p>
<p>Presenter: Isac Boström</p>
</body>
</html>
